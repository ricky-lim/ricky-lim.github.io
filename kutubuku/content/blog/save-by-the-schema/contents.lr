title: Save by the Schema
---
author: Ricky Lim
---
pub_date: 2025-06-25
---
body:

A schema is to represent the expected structure, and types of your data.
It's like a blueprint that defines how your data is shaped, what type each field is.

Here, we're going to explore schema with pyspark.

## Why Is Schema Important ?

### Data Integrity and consistency

A well-defined schema is like ahead-of-time rules on your data, what types are allowed, which fields are required.
This keeps your data reliable and if inconsistencies happen, schema can help us to fail fast.
This "fail fast" is essential for early detection of upstream issues, saving time, and avoiding downstream data corruption.

### Efficient selective ingestion

Defining a schema allows us to load only the fields that matter, skipping irrelevant data.
By doing less ingestion, we can speed up the process and save on storage costs.

## Practical Example: Defining Schema for Iris dataset

Here we have Iris dataset in JSON format that looks like this:

```JSON
{"measurement": {"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}, "species": "setosa"}
{"measurement": {"sepal_length": 4.9, "sepal_width": 3.0, "petal_length": 1.4, "petal_width": 0.2}, "species": "setosa"}
{"measurement": {"sepal_length": 4.7, "sepal_width": 3.2, "petal_length": 1.3, "petal_width": 0.2}, "species": "setosa"}
{"measurement": {"sepal_length": 4.6, "sepal_width": 3.1, "petal_length": 1.5, "petal_width": 0.2}, "species": "setosa"}
{"measurement": {"sepal_length": 5.0, "sepal_width": 3.6, "petal_length": 1.4, "petal_width": 0.2}, "species": "setosa"}
```

Let's define the schema for our iris dataset.

```python
measurement_schema = T.StructType(
    [
        T.StructField("sepal_length", T.DoubleType()),
        T.StructField("sepal_width", T.DoubleType()),
        T.StructField("petal_length", T.DoubleType()),
        T.StructField("petal_width", T.DoubleType()),
    ]
)

iris_schema = T.StructType(
    [
        T.StructField("measurement", measurement_schema),
        T.StructField("species", T.StringType()),
    ]
)
```

Now we can read our iris json file.

```python
iris = spark.read.json("iris.json", schema=iris_schema, multiLine=True, mode="FAILFAST")
iris.printSchema()
 |-- measurement: struct (nullable = true)
 |    |-- sepal_length: double (nullable = true)
 |    |-- sepal_width: double (nullable = true)
 |    |-- petal_length: double (nullable = true)
 |    |-- petal_width: double (nullable = true)
 |-- species: string (nullable = true)
```


We can also convert it as a flat table.

```python

iris_df = iris.select("measurement.*", "species")

iris_df.show(5)
+------------+-----------+------------+-----------+-------+
|sepal_length|sepal_width|petal_length|petal_width|species|
+------------+-----------+------------+-----------+-------+
|         5.1|        3.5|         1.4|        0.2| setosa|
|         4.9|        3.0|         1.4|        0.2| setosa|
|         4.7|        3.2|         1.3|        0.2| setosa|
|         4.6|        3.1|         1.5|        0.2| setosa|
|         5.0|        3.6|         1.4|        0.2| setosa|
+------------+-----------+------------+-----------+-------+
```

With this schema if there is any invalid type, as such in `invalid_iris.json`:

```json
{"measurement": {"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}, "species": "setosa"}
{"measurement": {"sepal_length": 4.9, "sepal_width": 3.0, "petal_length": 1.4, "petal_width": 0.2}, "species": "setosa"}
{"measurement": {"sepal_length": "a", "sepal_width": 3.2, "petal_length": 1.3, "petal_width": 0.2}, "species": "setosa"}
```

When we read this data, it will fail with the following error:

```bash
...
Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.
Cannot parse the value 'a' of the field `sepal_length` as target spark data type Doubl
...
```

Also we can use schema to read a subset of measurement for example only for petal.

```python
petal_iris_schema = T.StructType(
    [
        T.StructField("measurement", T.StructType([
            T.StructField("petal_length", T.DoubleType()),
            T.StructField("petal_width", T.DoubleType()),
        ])),
        T.StructField("species", T.StringType()),
    ]
)
petal_iris = spark.read.json(
    "iris.jsonl", schema=petal_iris_schema, multiLine=False, mode="FAILFAST"
)

petal_iris.printSchema()
root
 |-- measurement: struct (nullable = true)
 |    |-- petal_length: double (nullable = true)
 |    |-- petal_width: double (nullable = true)
 |-- species: string (nullable = true)
```

# Key takeaways:

- Predictable data processing: defining schema allows us to have robust data processing without reading it twice, no infer schema so you have the control!
- Fail Fast: as a safety net if dataset changes
- Faster processing: selecting only relevant data speeds up processing and lowers our compute and storage costs
